# -*- coding: utf-8 -*-
"""Untitled34.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zHqcRbBwrH4nI4HFIk4JDrBj-Y3n3nDw
"""

from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

# Variable data
EPOCHS = 50
BATCH_SIZE = 64
MODEL_SAVE_PATH = "asl_cnn_model_29cls_rel"
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# Get data from my Gdrive
df = pd.read_csv('drive/MyDrive/asl_landmarks.csv')
print("Classes found:", sorted(df['label'].unique()))

labels = df['label'].astype(str)
X = df.drop(columns=['label']).values.astype(np.float32)

num_landmarks = 21
coords_per_landmark = 3
num_classes = len(sorted(labels.unique()))

# Possible labels
label_map = {l: i for i, l in enumerate(sorted(labels.unique()))}
y = labels.map(label_map).values

# Data-Preprocessing
def preprocess_sample(sample):
    pts = sample.reshape(num_landmarks, coords_per_landmark)
    ref = pts[0]                        # wrist anchor
    pts -= ref                          # make all points relative to wrist
    pts = pts[1:]                       # optionally remove wrist itself
    scale = np.max(np.linalg.norm(pts, axis=1))
    pts /= (scale + 1e-6)               # normalize scale
    return pts

X_proc = np.stack([preprocess_sample(s) for s in X])
y_onehot = tf.keras.utils.to_categorical(y, num_classes)

# Split
X_train, X_val, y_train, y_val = train_test_split(
    X_proc, y_onehot, test_size=0.1, random_state=RANDOM_SEED, stratify=y
)


def conv_block(x, filters, kernel_size=3, drop=0.2):
    y = layers.Conv1D(filters, kernel_size, padding='same', use_bias=False)(x)
    y = layers.BatchNormalization()(y)
    y = layers.Activation('relu')(y)
    y = layers.Dropout(drop)(y)
    return y

def residual_block(x, filters, kernel_size=3, drop=0.2):
    shortcut = x
    y = conv_block(x, filters, kernel_size, drop)
    y = layers.Conv1D(filters, kernel_size, padding='same', use_bias=False)(y)
    y = layers.BatchNormalization()(y)
    if shortcut.shape[-1] != filters:
        shortcut = layers.Conv1D(filters, 1, padding='same', use_bias=False)(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)
    y = layers.Add()([shortcut, y])
    y = layers.Activation('relu')(y)
    y = layers.Dropout(drop)(y)
    return y

def build_thicc_cnn(input_shape=(20, 3), num_classes=29, dropout=0.3):
    inp = layers.Input(shape=input_shape)
    x = conv_block(inp, 64, drop=dropout)
    x = conv_block(x, 128, drop=dropout)
    x = residual_block(x, 128, drop=dropout)
    x = residual_block(x, 256, drop=dropout)
    x = residual_block(x, 256, drop=dropout)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(512, use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    return models.Model(inp, out)

model = build_thicc_cnn((20, 3), num_classes)

# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),
    metrics=['accuracy']
)

# Callbacks for optimal training
cbs = [
    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),
    callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
    callbacks.ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True, verbose=1)
]

# Training
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=cbs,
    verbose=1
)

# Final model save
model.save(MODEL_SAVE_PATH)
print(f"Model saved to {MODEL_SAVE_PATH}")

# This code was from google colab
